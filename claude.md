# Sugarcane Disease Classification System

**Vision System**: Image classification using YOLOv11n-cls for mobile/edge deployment
**Dataset**: [Kaggle Sugarcane Leaf Disease Dataset](https://www.kaggle.com/datasets/nirmalsankalana/sugarcane-leaf-disease-dataset)

## Dataset Overview

- **Classes**: 5 disease categories
  - Healthy
  - Mosaic
  - Redrot
  - Rust
  - Yellow
- **Total Images**: 2,569 (balanced distribution)
- **Source**: Maharashtra, India (real-world conditions)
- **Size**: 167.3 MB

## Quick Start

### 1. Setup Kaggle API
```bash
# Get API credentials from https://www.kaggle.com/settings
# Download kaggle.json and place in ~/.kaggle/
pip install kagglehub ultralytics
```

### 2. Download Dataset
```python
import kagglehub
path = kagglehub.dataset_download("nirmalsankalana/sugarcane-leaf-disease-dataset")
print("Path to dataset files:", path)
```

### 3. Prepare Dataset
```bash
python scripts/prepare_dataset.py --source <kaggle_path> --dest dataset/
```

### 4. Train in Google Colab
```python
from ultralytics import YOLO

# Load YOLOv11n-cls (nano model for mobile)
model = YOLO('yolov11n-cls.pt')

# Train
results = model.train(
    data='dataset',
    epochs=100,
    imgsz=224,
    batch=64,
    patience=15,
    device=0  # GPU
)

# Validate
metrics = model.val()

# Export to TFLite for mobile
model.export(format='tflite')
```

## Training Workflow

**Phase 1: Data Preparation**
- Download dataset using kagglehub
- Organize into train/val/test splits (70/20/10)
- Structure: `dataset/{train,val,test}/{class_name}/`

**Phase 2: Training (Google Colab)**
- Model: YOLOv11n-cls (optimized for mobile/edge)
- Image size: 224x224
- Augmentation: Auto-enabled (rotation, scaling, color jitter)
- Expected accuracy: 90-95%
- Training time: ~2-4 hours on Colab GPU

**Phase 3: Evaluation**
- Metrics: Accuracy, Precision, Recall, F1-Score
- Confusion matrix analysis
- Per-class performance review

**Phase 4: Export & Deployment**
- Export to TFLite (Android/iOS/Raspberry Pi)
- Export to ONNX (backup/web deployment)
- Quantization: INT8 for smaller model size

## Model Selection

**YOLOv11n-cls** (Recommended for mobile/edge)
- **Size**: ~5 MB
- **Speed**: 30-60 FPS on mobile
- **Accuracy**: 90-95% expected
- **Deployment**: Optimized for edge devices

## Deployment Guide

### Mobile (TFLite)
```python
from ultralytics import YOLO

# Load exported model
model = YOLO('best.tflite')

# Inference
results = model('sugarcane_leaf.jpg')
predicted_class = results[0].probs.top1
confidence = results[0].probs.top1conf
```

### Integration
- **Android**: Use TFLite Android library
- **iOS**: Use TFLite iOS/CoreML
- **Raspberry Pi**: Direct TFLite inference
- **Web**: Convert to ONNX, use ONNX.js

## Project Structure

```
scvision/
├── claude.md                 # This file
├── requirements.txt          # Python dependencies
├── dataset/                  # Organized dataset
│   ├── train/               # Training images (70%)
│   ├── val/                 # Validation images (20%)
│   └── test/                # Test images (10%)
├── scripts/
│   └── prepare_dataset.py   # Dataset organization script
├── notebooks/
│   └── train_colab.ipynb    # Google Colab training notebook
├── models/                   # Trained model weights
└── inference.py             # Example inference script
```

## Key Configuration

**data.yaml** (auto-generated by prepare_dataset.py)
```yaml
path: /path/to/dataset
train: train
val: val
test: test
names:
  0: Healthy
  1: Mosaic
  2: Redrot
  3: Rust
  4: Yellow
```

## Performance Expectations

Based on similar agricultural disease detection research:
- **Accuracy**: 90-95%
- **Precision/Recall**: 0.90-0.95
- **F1-Score**: 0.90-0.95
- **Inference Speed**: 30-60 FPS on mobile GPU
- **Model Size**: ~5 MB (TFLite)

## Best Practices

1. **Data Augmentation**: Critical for ~513 images/class
   - Rotation (±30°)
   - Scaling (0.8-1.2x)
   - Color jitter
   - Horizontal/vertical flips

2. **Training**:
   - Use early stopping (patience=15)
   - Monitor validation metrics
   - Save best model based on val accuracy

3. **Deployment**:
   - INT8 quantization for mobile (3-4x speedup)
   - Test on target device before production
   - Implement confidence threshold (e.g., 0.7)

## Common Issues & Solutions

**Issue**: Low accuracy (<85%)
- Solution: Increase epochs, add more augmentation, try YOLOv11s-cls

**Issue**: Overfitting (train acc >> val acc)
- Solution: Stronger augmentation, reduce epochs, add dropout

**Issue**: Slow mobile inference
- Solution: Use INT8 quantization, reduce image size to 160x160

## Resources

- [Ultralytics YOLO Docs](https://docs.ultralytics.com)
- [YOLOv11 Classification](https://docs.ultralytics.com/tasks/classify/)
- [Model Export Guide](https://docs.ultralytics.com/modes/export/)
- [TFLite Integration](https://www.tensorflow.org/lite)

## Next Steps

1. ✅ Setup project structure
2. ✅ Download dataset
3. ✅ Organize dataset (run prepare_dataset.py)
4. ✅ Train model in Colab
5. ✅ Evaluate and export
6. ✅ Deploy to web (Vercel) with PWA support

## Future Enhancements

### Image Validation Layer

**Problem**: Currently, the classifier will attempt to classify ANY image as a sugarcane disease, even if the image is not a sugarcane leaf at all (e.g., cats, cars, random objects). This leads to nonsensical predictions and poor user experience.

**Requirement**: Add a validation layer to verify the uploaded image is actually a sugarcane leaf before running disease classification.

#### Approach A: Two-Stage Classification (Recommended for Production)

**Architecture**:
1. **Stage 1**: Binary classifier (Sugarcane Leaf vs Not Sugarcane)
2. **Stage 2**: Disease classifier (current YOLO model)

**Implementation**:
```python
# Stage 1: Train binary classifier
model_validator = YOLO('yolov11n-cls.pt')
results = model_validator.train(
    data='binary_dataset',  # sugarcane_leaf vs not_sugarcane
    epochs=50,
    imgsz=224,
    batch=64
)

# Stage 2: Use in inference
def classify_with_validation(image):
    # First check if it's a sugarcane leaf
    validation_result = model_validator(image)
    if validation_result.probs.top1 == 0:  # Not sugarcane
        return {"error": "Image does not appear to be a sugarcane leaf"}

    # Then classify the disease
    disease_result = disease_model(image)
    return disease_result
```

**Pros**:
- Fast inference (~30-60ms total on mobile)
- Works offline
- High accuracy (can achieve >95%)
- Small model size (~10 MB total for both models)
- No API costs

**Cons**:
- Requires collecting/curating "not sugarcane" dataset
- Need to train and maintain two models
- Slightly increased complexity

**Dataset Requirements**:
- **Positive class** (sugarcane_leaf): Use existing dataset images
- **Negative class** (not_sugarcane): Collect diverse images:
  - Other plant leaves (corn, wheat, rice, etc.)
  - Common objects (hands, tables, grass, etc.)
  - Random backgrounds
  - Suggested: ~2,000-5,000 negative examples

#### Approach B: Vision LLM for Validation (Experimental)

**Architecture**: Use a vision-language model to interpret the image before classification.

**Implementation Options**:

**Option B1: Cloud-based (GPT-4V, Claude Vision)**
```javascript
// Client-side validation using OpenAI Vision API
async function validateWithLLM(imageBase64) {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${API_KEY}`,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            model: "gpt-4-vision-preview",
            messages: [{
                role: "user",
                content: [
                    {
                        type: "text",
                        text: "Is this image a sugarcane leaf? Answer only 'yes' or 'no'."
                    },
                    {
                        type: "image_url",
                        image_url: { url: `data:image/jpeg;base64,${imageBase64}` }
                    }
                ]
            }],
            max_tokens: 10
        })
    });

    const result = await response.json();
    return result.choices[0].message.content.toLowerCase().includes('yes');
}
```

**Option B2: Local Vision LLM (LLaVA, MobileVLM)**
```python
# Run locally with ONNX-converted vision LLM
from transformers import AutoProcessor, AutoModelForVision2Seq

processor = AutoProcessor.from_pretrained("llava-hf/llava-1.5-7b-hf")
model = AutoModelForVision2Seq.from_pretrained("llava-hf/llava-1.5-7b-hf")

def validate_with_local_llm(image):
    prompt = "Is this a sugarcane leaf? Answer yes or no."
    inputs = processor(prompt, image, return_tensors="pt")
    output = model.generate(**inputs, max_new_tokens=10)
    response = processor.decode(output[0], skip_special_tokens=True)
    return "yes" in response.lower()
```

**Pros**:
- No training required
- Can provide detailed explanations ("This is a cat, not a plant")
- Highly accurate for diverse inputs
- Easy to update prompts without retraining

**Cons**:
- **Cloud API**: Costs money per request (~$0.01-0.05/image)
- **Cloud API**: Requires internet connection
- **Cloud API**: Privacy concerns (uploading images)
- **Local LLM**: Large model size (5-15 GB)
- **Local LLM**: Slow inference on mobile (1-5 seconds)
- **Local LLM**: High memory requirements

#### Approach C: Hybrid (Best of Both Worlds)

**Architecture**: Use fast binary classifier as primary filter, fall back to LLM for edge cases.

```javascript
async function smartValidation(image) {
    // Fast check with binary classifier
    const quickCheck = await binaryClassifier(image);

    if (quickCheck.confidence > 0.95) {
        // High confidence, trust the result
        return quickCheck.isSugarcane;
    }

    // Low confidence, use LLM for verification
    const llmCheck = await validateWithLLM(image);
    return llmCheck;
}
```

**Pros**:
- Fast for clear cases (95% of images)
- Accurate for ambiguous cases
- Cost-effective (only pay for uncertain images)

**Cons**:
- Most complex to implement
- Requires both solutions

### Recommended Implementation Plan

**Phase 1: MVP** (1-2 weeks)
- Collect "not sugarcane" dataset (~2,000 images)
- Train binary YOLOv11n-cls validator
- Integrate into web app with confidence threshold (0.7)
- Add user-friendly error messages

**Phase 2: Enhancement** (optional)
- Add LLM fallback for edge cases
- Collect failed validation examples to improve dataset
- Fine-tune confidence thresholds based on user feedback

**Phase 3: Advanced** (future)
- Multi-class validator (sugarcane vs other crops vs non-plant)
- Active learning pipeline to continuously improve
- Explain validation results to users

### Technical Specifications

**Confidence Thresholds**:
- Reject image if sugarcane confidence < 0.7
- Show warning if 0.7 < confidence < 0.85
- Proceed normally if confidence > 0.85

**Error Messages** (Chinese):
```javascript
const validationErrors = {
    notSugarcane: "检测到的图像不是甘蔗叶。请上传甘蔗叶片的照片。",
    lowConfidence: "无法确定这是否是甘蔗叶。请尝试：\n• 使用更好的光线\n• 确保叶片清晰可见\n• 避免背景杂乱",
    tooBlurry: "图像太模糊。请重新拍摄清晰的照片。"
};
```

**Performance Targets**:
- Validation latency: < 100ms (binary classifier)
- False positive rate: < 5% (incorrectly accepting non-sugarcane)
- False negative rate: < 2% (incorrectly rejecting sugarcane)

### Resources

- [YOLO Binary Classification](https://docs.ultralytics.com/tasks/classify/)
- [GPT-4V API Docs](https://platform.openai.com/docs/guides/vision)
- [LLaVA: Large Language and Vision Assistant](https://llava-vl.github.io/)
- [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html) (for negative examples)
